["Unit 2: Supervised Learning\nPart 1\nDr. Goonjan Jain\nMathematics and Computing\nDepartment of Applied Mathematics\nDTU\nContents\n\u2022 K-nearest neighbours\n\u2022 Bayesian Classification\n\u2022 Na\u00efve Bayes\n\u2022 Linear Discriminant Analysis\n\u2022 Linear Regression\n\u2022 Logistic Regression\n\u2022 Multilayer Perceptron\n\u2022 Support Vector Machine\n\u2022 Decision Trees\nK-nearest neighbours\n\u2022 K-Nearest Neighbours (KNN) is a supervised learning algorithm. \u2022 It is used for both classification and regression tasks. \u2022 The algorithm works based on similarity between data points. \u2022 also called lazy learner algorithm\n\u2022 does not learn from the training set immediately instead it stores the \nentire dataset and performs computations only at the time of \nclassification. Why KNN? \u2022 Simple and intuitive algorithm.", "ple because most of its neighbors are apples. Choosing the Value of K\n\u2022 Small K: Sensitive to noise. \u2022 No explicit training phase. \u2022 Works well with small datasets. \u2022 Easy to understand and implement. How KNN Works\n\u2022 Choose the value of K.\n\u2022 Calculate distance between test point and training points. \u2022 Select K nearest neighbors. \u2022 Assign class by majority voting. What is \u2018K\u2019? \u2022 a number that tells the algorithm how many nearby points or \nneighbors to look at when it makes a decision. \u2022 deciding which fruit it is based on its shape and size. \u2022 compare it to fruits you already know. \u2022 If k = 3, the algorithm looks at the 3 closest fruits to the new one. \u2022 If 2 of those 3 fruits are apples and 1 is a banana, the algorithm says \nthe new fruit is an apple because most of its neighbors are apples. Choosing the Value of K\n\u2022 Small K: Sensitive to noise.", "choice \nfor k.\n\u2022 Odd Values for k: \n\u2022 use an odd number for k especially in classification problems. \u2022 Large K: Smoother decision boundary. \u2022 Optimal K is chosen using cross-validation. Statistical Methods for Selecting k\n\u2022 Cross-Validation:\n\u2022 use k-fold cross-validation. \u2022 divide the dataset into k parts\n\u2022 model is trained on some of these parts and tested on the remaining ones\n\u2022 process is repeated for each part\n\u2022 The k value that gives the highest average accuracy during these tests is usually the best one to \nuse. \u2022 Elbow Method: \n\u2022 draw a graph showing the error rate or accuracy for different k values. \u2022 As k increases the error usually drops at first. But after a certain point error stops decreasing \nquickly. \u2022 The point where the curve changes direction and looks like an \"elbow\" is usually the best choice \nfor k.\n\u2022 Odd Values for k: \n\u2022 use an odd number for k especially in classification problems.", "\u2192 {High, Normal}\n\u2022 Body Ache (Ba) \u2192 {Yes, No}\n\u2022\n Important:\nFever and Body Ache are not independent. \u2022 helps avoid ties when deciding which class is the most common among the neighbors. Distance Metrics\n\u2022 Euclidean Distance\n\u2022 straight-line distance between two points in a plane or space\n\u2022 Manhattan Distance\n\u2022 total distance you would travel if you could only move along horizontal and vertical \nlines like a grid or city streets\n\u2022 Minkowski Distance\n\u2022 a family of distances, which includes both Euclidean and Manhattan distances as \nspecial cases. \u2022 Euclidean distance is most commonly used. Python Code using library\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n# Training data: [hours studied, attendance %]\nX = np.array([[2, 60],    [4, 70],    [6, 80],    [8, 90],    [1, 50],    [9, 95]])\n# Labels: 0 = Fail, 1 = Pass\ny = np.array([0, 0, 1, 1, 0, 1])\n# Create KNN model with k = 3\nknn = KNeighborsClassifier(n_neighbors=3)\n# Train the model\nknn.fit(X, y)\n# New student data\nnew_student = np.array([[5, 75]])\n# Prediction\nprediction = knn.predict(new_student)\nprint(\"Prediction:\", \"Pass\" if prediction[0] == 1 else \"Fail\")\nPython Code\nimport math\n# Training data: [hours studied, attendance %]\nX_train = [[2, 60],    [4, 70],    [6, 80],    [8, 90],    [1, 50],    [9, 95]]\n# Labels: 0 = Fail, 1 = Pass\ny_train = [0, 0, 1, 1, 0, 1]\n# New data point\ntest_point = [5, 75]\nk = 3\n# Step 1: Euclidean distance function\ndef euclidean_distance(p1, p2):\nreturn math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n# Step 2: KNN function\ndef knn_predict(X, y, test, k):\ndistances = []\n# Calculate distance from test point to all training points\nfor i in range(len(X)):\ndist = euclidean_distance(X[i], test)\ndistances.append((dist, y[i]))\n# Sort distances\ndistances.sort(key=lambda x: x[0])\n# Take k nearest neighbors\nnearest_neighbors = distances[:k]\n# Majority voting\nvotes = [label for (_, label) in nearest_neighbors]\nprediction = max(set(votes), key=votes.count)\nreturn prediction\n# Make prediction\nresult = knn_predict(X_train, y_train, test_point, k)\nprint(\"Prediction:\", \"Pass\" if result == 1 else \"Fail\")\nAdvantages and Disadvantages\n\u2022 Advantages:\n\u2022 Simple and effective\n\u2022 No training phase\n\u2022 Disadvantages:\n\u2022 Slow for large datasets\n\u2022 Sensitive to irrelevant features\nApplications of KNN\n\u2022 Pattern recognition\n\u2022 Recommendation systems\n\u2022 Medical diagnosis\n\u2022 Image and text classification\nBayesian Classification\n\u2022 Real-world data is uncertain\n\u2022 Decisions should consider probability, not just rules\n\u2022 Bayesian methods provide:\n\u2022 Mathematical foundation\n\u2022 Interpretability\n\u2022 A probabilistic classifier\n\u2022 Assigns a class label based on:\n\u2022 Prior probability of a class\n\u2022 Probability of observing data given the class\n\u2022 Uses Bayes\u2019 Theorem\n\u2022 Works well when:\n\u2022 Uncertainty is present\n\u2022 Prior knowledge is useful\nBayes\u2019 Theorem \u2013 Core Formula\nWhere:\n\u2022 X \u2013 input data\n\u2022 C - class\n\u2022 \ud835\udc43 \ud835\udc36 \u2223 \ud835\udc4b  :Posterior probability - Updated belief after seeing data\n\u2022 \ud835\udc43 \ud835\udc4b \u2223 \ud835\udc36  :Likelihood - How likely data is for a class\n\u2022 \ud835\udc43 \ud835\udc36  :Prior probability - What we believe before seeing data\n\u2022 \ud835\udc43 \ud835\udc4b  :Evidence\n\u2022 Choose class C with maximum \ud835\udc43 \ud835\udc36 \u2223 \ud835\udc4b\n\nBayesian Classification Idea\n\u2022 Compute probability for each class\n\u2022 Assign the class with highest posterior probability\n\u2022 Decision Rule: \n\ud835\udc36\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc43 \ud835\udc36 \u2223 \ud835\udc4b\nSimple Example \u2013 Weather & Play Tennis\n\u2022 Given:\n\u2022 Classes: Play / Not Play\n\u2022 Input: Weather = Sunny\n\u2022 We compute:\n\u2022 P(Play | Sunny)\n\u2022 P(Not Play | Sunny)\n\u2022 Choose the higher value\n\u2022 Assume:\n\u2022 P(Play) = 0.6\n\u2022 P(Not Play) = 0.4\n\u2022 P(Sunny | Play) = 0.3\n\u2022 P(Sunny | Not Play) = 0.5\n\u2022 Compute posterior probabilities\n\ud835\udc43 \ud835\udc43\ud835\udc59\ud835\udc4e\ud835\udc66 \u2223 \ud835\udc46\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 = 0.3 \u00d7 0.6 = 0.18\n\ud835\udc43 \ud835\udc41\ud835\udc5c\ud835\udc61\ud835\udc43\ud835\udc59\ud835\udc4e\ud835\udc66 \u2223 \ud835\udc46\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 = 0.5 \u00d7 0.4 = 0.20\n\u2022 Prediction: Not Play\nExample: Disease Diagnosis using Bayesian \nClassification\n\u2022 Problem Statement\n\u2022 A doctor wants to diagnose whether a patient has Flu (F) or Allergy \n(A) based on two features:\n\u2022 Fever (Fv) \u2192 {High, Normal}\n\u2022 Body Ache (Ba) \u2192 {Yes, No}\n\u2022\n Important:\nFever and Body Ache are not independent.", "ption\n\u2022 The presence of the words \u201cOffer\u201d and \u201cFree\u201d are conditionally \nindependent given the class. If fever is high, body ache is more likely. Step 1: Prior Probabilities\n\u2022 From hospital data:\nDisease Probability\nFlu (F) P(F) = 0.6\nAllergy (A) P(A) = 0.4\nStep 2: Joint Conditional Probabilities\n\u2022 Given Flu Given Allergy\nFever Body Ache P(Fv,Ba| F)\nHigh Yes 0.50\nHigh No 0.10\nNormal Yes 0.25\nNormal No 0.15\nFever Body Ache P(Fv,Ba| A)\nHigh Yes 0.05\nHigh No 0.15\nNormal Yes 0.10\nNormal No 0.70\nStep 3: New Patient (Test Case)\n\u2022 A patient has:\n\u2022 Fever = High\n\u2022 Body Ache = Yes\n\u2022 We want to find:\nP(Flu | High fever, Body Ache)\nP(Allergy | High fever, Body Ache)\nStep 4: Bayesian Classification\n\u2022 For Flu:\nP(F | Fv = High, Ba = Yes) \n= P(F). P(Fv = High, Ba = Yes | F)\n= 0.50 * 0.6 = 0.30\n\u2022 For Allergy:\nP(A | Fv = High, Ba = Yes) \n= P(A). P(Fv = High, Ba = Yes | A)\n= 0.05 * 0.4 = 0.02\nPredicted Disease: Flu\nClass Posterior Score\nFlu 0.30\nAllergy 0.02\nAdvantages of Bayesian Classification\n\u2022 Strong theoretical foundation\n\u2022 Works well with small datasets\n\u2022 Handles uncertainty\nLimitations\n\u2022 Requires probability estimation\n\u2022 Can be computationally expensive\n\u2022 Assumptions may not always hold\nSummary \u2013 Bayesian Classification\n\u2022 Bayesian classification uses probability\n\u2022 Based on Bayes\u2019 theorem\n\u2022 Foundation for Naive Bayes\nNa\u00efve Bayes\n\u2022 A special case of Bayesian classification\n\u2022 Assumes conditional independence among features\n\u2022 Words in an email are independent\n\u2022 This assumption simplifies computation\n\u2022 Surprisingly effective in practice\n\u2022 For a data point\n\ud835\udc4b = (x1, x2, x3 \u2026, xn)\nThe predicted class is:\n\u1218\ud835\udc36 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65\n\ud835\udc36\n\ud835\udc43(\ud835\udc36) \u0dd1\n\ud835\udc56=1\n\ud835\udc5b\n\ud835\udc43(\ud835\udc65\ud835\udc56|\ud835\udc36)\nWhere:\n\u2022 \ud835\udc43 \ud835\udc36  = Prior probability\n\u2022 \ud835\udc43 \ud835\udc65\ud835\udc56 \u2223 \ud835\udc36  = Likelihood\n\u2022 Independence assumption applies\nExample - Email Spam Detection using Naive \nBayes\n\u2022 Problem Statement\n\u2022 We want to classify an email as Spam (S) or Not Spam (N) using two \nfeatures:\n\u2022 Contains word \u201cOffer\u201d (O) \u2192 {Yes, No}\n\u2022 Contains word \u201cFree\u201d (F) \u2192 {Yes, No}\n\u2022 Naive Assumption\n\u2022 The presence of the words \u201cOffer\u201d and \u201cFree\u201d are conditionally \nindependent given the class.", "Large Small\nBayesian Classification models reality more accurately but is computationally expensive. \u2022 This assumption is what differentiates Naive Bayes from full \nBayesian classification. Step 1: Prior Probabilities\n\u2022 From historical email data\nClass Probability\nSpam (S) P(S) = 0.5\nNot Spam (N) P(N) = 0.5\nStep 2: Feature Probabilities (Independent)\n\u2022 Given spam Given Not Spam\nFeature Probability\nP(Offer=Yes | S) 0.7\nP(Free=Yes | S) 0.8\nFeature Probability\nP(Offer=Yes | N) 0.2\nP(Free=Yes | N) 0.1\nStep 3: New Email (Test Case)\n\u2022 Email contains:\n\u2022 Offer = Yes\n\u2022 Free = Yes\nStep 4: Naive Bayes Calculation\n\u2022 For Spam:\nP(S| O = Yes, F = Yes)\n= P(S) * P(O = Yes | S) * P(F = Yes | S)\n= 0.5 * 0.7 * 0.8 \n= 0.28\n\u2022 For Not Spam:\nP(N| O = Yes, F = Yes)\n= P(N) * P(O = Yes | N) * P(F = Yes | N)\n= 0.5 * 0.2 * 0.1 \n= 0.01\nFinal Decision\n\u2022 Final Decision\n\u2022\n Predicted Class: Spam\nClass Score\nSpam 0.28\nNot Spam 0.01\nBayesian vs Na\u00efve Bayes\nAspect Bayesian Classification Naive Bayes\nFeature dependence Allowed Not allowed\nProbability used Joint Product of marginals\nAccuracy Higher (if dependencies exist) Lower\nComplexity High Very low\nData needed Large Small\nBayesian Classification models reality more accurately but is computationally expensive.", "dundant\n\u2022 Question: Can we project data into a lower dimension while \npreserving class separability? Naive Bayes simplifies reality for speed and scalability. Types of Na\u00efve Bayes\n\u2022 Gaussian Naive Bayes \u2013 continuous data\n\u2022 Multinomial Naive Bayes \u2013 text data\n\u2022 Bernoulli Naive Bayes \u2013 binary features\n\u2022 Naive Bayes classifiers differ based on how they model the feature \nprobabilities \ud835\udc43 \ud835\udc65\ud835\udc56 \u2223 \ud835\udc36 . Gaussian Na\u00efve Bayes\n\u2022 When features are continuous (real-valued)\n\u2022 Data roughly follows a normal (Gaussian) distribution\n\u2022 Examples:\n\u2022 Height, weight, temperature\n\u2022 Marks, sensor readings\n\u2022 Medical measurements\n Assumption\n\u2022 Each feature follows a Gaussian (normal) distribution for each class. Probability Model\nWhere:\n\ud835\udf07= mean of the feature in a class\n\ud835\udf0e2 = variance of the feature in a class\n\nMultinomial Na\u00efve Bayes\n\u2022 When features represent counts or frequencies\n\u2022 Especially good for text classification\n\u2022 Examples:\n\u2022 Word counts in documents\n\u2022 Email spam detection\n\u2022 Sentiment analysis\n Assumption\n\u2022 Features follow a multinomial distribution\n\u2022 Counts matter (how many times a word appears)\n Intuition\n\u2022 If a word appears frequently in spam emails, it increases spam probability\n\u2022 Repeated words strengthen the decision\nWhere:\n\ud835\udc41\ud835\udc36,\ud835\udc56 =count of feature \ud835\udc56in class \ud835\udc36\n\ud835\udc41\ud835\udc36 =total feature count in class \ud835\udc36\n\ud835\udc51= number of features (vocabulary size)\n\ud835\udefc= smoothing parameter (Laplace smoothing)\n\nBernoulli Na\u00efve Bayes\n\u2022 When features are binary (0 or 1)\n\u2022 Presence or absence matters\n\u2022 Examples:\n\u2022 Word appears or not\n\u2022 Yes/No features\n\u2022 Clicked or not clicked\n Assumption\n\u2022 Features follow a Bernoulli distribution\n\u2022 Only presence or absence is important\nPython Code \u2013 Gaussian Na\u00efve Bayes\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np\n# Training data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([0, 0, 1, 1, 1])\nmodel = GaussianNB()\nmodel.fit(X, y)\n# Prediction\nprint(model.predict([[2]]))\nPython Code: Multinomial Naive Bayes\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\ntexts = [\"free money\", \"win prize\", \"hello friend\", \"how are you\"]\nlabels = [1, 1, 0, 0]\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(texts)\nmodel = MultinomialNB()\nmodel.fit(X, labels)\nprint(model.predict(vectorizer.transform([\"free prize\"])) )\nAdvantages\n\u2022 Very fast\n\u2022 Works well for text data\n\u2022 Needs less training data\nLimitations\n\u2022 Independence assumption is unrealistic\n\u2022 Poor performance with correlated features\nKey Takeaways\n\u2022 Naive Bayes is simple yet powerful\n\u2022 Based on Bayes\u2019 theorem\n\u2022 Widely used in NLP and spam detection\nLinear Discriminant Analysis\n\u2022 In many ML problems:\n\u2022 Data has multiple features\n\u2022 Visualizing and classifying becomes difficult\n\u2022 Some features may be irrelevant or redundant\n\u2022 Question: Can we project data into a lower dimension while \npreserving class separability?", "tance between class means \n\u2022 Minimum spread within each class\nExample\n\u2022 Each point is a data sample. \u2022\n  Answer: Linear Discriminant Analysis (LDA)\nLDA - Analogy\n\u2022\n  Imagine shooting arrows at two targets\n\u2022 Target A = Class 1\n\u2022 Target B = Class 2\n\u2022 Bad scenario:\n\u2022 Arrows are spread everywhere\n\u2022 Targets are close\n\u2022 Good scenario:\n\u2022 Arrows tightly grouped\n\u2022 Targets far apart\n\u2022\n  LDA moves and rotates the camera to get the clearest view where \ntargets look far apart and compact. LDA as a smart projection\n\u2022 Instead of working in 2D or 3D\u2026\n\u2022 LDA asks:\n\u2022 \u201cIf I project all points onto one-line, which line will make the classes easiest to \nseparate?\u201d\n\u2022 Bad projection \n\u2022 Classes overlap heavily\n\u2022 Good projection \n\u2022 Classes form separate clusters\n\u2022\n  LDA finds the best possible line\nWhat does LDA actually do? \u2022 LDA does two things at the same time:\n\u2022 Push different classes far apart\n\u2022 Pull data points of the same class close together\n\u2022\n  Best separation =\n\u2022 Maximum distance between class means \n\u2022 Minimum spread within each class\nExample\n\u2022 Each point is a data sample.", "s are possible, but most of them do a poor job of separating classes.\u201d\n Which line should we choose? \u2022 X-axis \u2192 Feature 1\n\u2022 Y-axis \u2192 Feature 2\n\u2022 Two classes:\n\u2022 Class 0 (left cluster)\n\u2022 Class 1 (right cluster)\n\u2022 Key observation to tell students\n\u2022 The data is 2-dimensional\n\u2022 The classes are somewhat separable, but not by a single obvious axis\n\u2022 We need a good direction to project data so that the classes are best separated\n\u2022 \u201cAt this stage, we only have data. No model, no line, no intelligence yet.\u201d\n\nStep 2: Possible Lines (Bad Choices)\nWhat the plot shows\n\u2022Same data as before\n\u2022Three different arbitrary lines\n\u2022These lines represent random projection directions\nWhy these lines are NOT good\n\u2022They cut through both classes\n\u2022When data is projected onto these lines:\n\u2022 Class overlap is high\n\u2022 Classification becomes difficult\n\u2022They do not maximize class separation\n\u201cMany lines are possible, but most of them do a poor job of separating classes.\u201d\n Which line should we choose?", "ance structure. \u2022 Linear Separability: The data should be separable using a straight line \nor plane. LDA chooses a direction that:\n1.Maximizes distance between class means\n2.Minimizes spread within each class\nWhy this is a great choice\n\u2022The line passes through class centers\n\u2022Projection onto this line:\n\u2022 Pulls same-class points close together\n\u2022 Pushes different-class points far apart\n\u2022This gives maximum class separability\n\u201cLDA finds the line along which the classes look most different.\u201d\nWhat LDA does? Key Assumptions\n\u2022 For LDA to perform effectively, certain assumptions are made:\n\u2022 Gaussian Distribution: The data in each class should follow a normal \nbell-shaped distribution. \u2022 Equal Covariance Matrices: All classes should have the same \ncovariance structure. \u2022 Linear Separability: The data should be separable using a straight line \nor plane.", " 1: Compute class means\n\u2022 Mean of Class 1\n\u2022 Mean of Class 2\n\u2022 These are the \u201ccenters\u201d of each class. \u2022 For these reasons, LDA may not perform well in high-dimensional \nfeature spaces. Where is LDA Used? \u2022 Face recognition \n\u2022 Medical diagnosis (disease vs healthy)\n\u2022 Spam vs non-spam classification\n\u2022 Student performance classification\nLDA\n\u2022 LDA wants to maximize the ratio:\n\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 \ud835\udc4f\ud835\udc52\ud835\udc61\ud835\udc64\ud835\udc52\ud835\udc52\ud835\udc5b \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60 \ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc60\n\ud835\udc46\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51 \ud835\udc64\ud835\udc56\ud835\udc61\u210e\ud835\udc56\ud835\udc5b \ud835\udc52\ud835\udc4e\ud835\udc50\u210e \ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\n\u2022 Large numerator \u2192 classes far apart\n\u2022 Small denominator \u2192 classes tight\n\u2022 This single idea drives the entire algorithm. Key Quantities in LDA\n\u2022 LDA works using:\n\u2022 Mean of each class\n\u2022 Within-class scatter\n\u2022 Between-class scatter\n\u2022 These help quantify:\n\u2022 Compactness of each class\n\u2022 Separation between classes\nHow LDA works internally\n\u2022 Step 1: Compute class means\n\u2022 Mean of Class 1\n\u2022 Mean of Class 2\n\u2022 These are the \u201ccenters\u201d of each class.", " \u201c0\u201d\n\u2022 Class \u03c9\u2082: Image of digit \u201c1\u201d\n\u2022 Each image is processed and converted into numerical features. \u2022 Step 2: Measure how spread out each class is\n\u2022 Are points tightly packed? \u2022 Or scattered? \u2022 This is called within-class scatter. \u2022 Step 3: Measure how far apart the classes are\n\u2022 Distance between class means\n\u2022 This is between-class scatter. \u2022 Step 4: Find the direction that:\n\u2022\n  Maximizes distance between means\n Minimizes spread inside classes\n\u2022 That direction = LDA projection axis\n\u2022 Step 5: Project data onto this axis\n\u2022 Now classification becomes easy:\n\u2022 Just check which side of the line a point falls on\nExample \u2013 Image Classification\n\u2022 Suppose we want to classify grayscale images of handwritten \ncharacters into two classes:\n\u2022 Class \u03c9\u2081: Image of digit \u201c0\u201d\n\u2022 Class \u03c9\u2082: Image of digit \u201c1\u201d\n\u2022 Each image is processed and converted into numerical features.", "ility\n\u2022 Useful for both visualization and classification\n\u2022 Widely used in real-world ML applications Step 1: Feature Extraction from Images\n\u2022 Instead of using raw pixels, we extract meaningful features from each image. \u2022 Let each image be represented by 3 features:\n\u2022 So, each image = a 3-dimensional feature vector\n\u2022 \ud835\udc31 =\n\ud835\udc651\n\ud835\udc652\n\ud835\udc653\nFeature Description\nx\u2081 Average pixel intensity\nx\u2082 Vertical symmetry score\nx\u2083 Edge density\nStep 2: Training Data (Multiple Samples)\nSample x\u2081 x\u2082 x\u2083\n1 6 5 2\n2 7 6 3\n3 8 5 2\n4 7 7 3\nSample x\u2081 x\u2082 x\u2083\n1 2 3 7\n2 3 2 6\n3 2 4 8\n4 3 3 7\nClass \u03c9\u2081 (Digit \u201c0\u201d) Class \u03c9\u2082 (Digit \u201c1\u201d)\nStep 3: Compute Class Mean Vectors\n\u2022 Mean of Class \u03c9\u2081\n\ud835\udf071 =\n7\n5.75\n2.5\n\u2022 Mean of Class \u03c9\u2082\n\ud835\udf072 =\n2.5\n3\n7\nStep 4: Compute Within-Class Scatter Matrix \n(Sw)\n\u2022 Definition\n\u2022 This measures how spread out each class is internally. \u2022 Contains variances on the diagonal\n\u2022 Contains covariances off the diagonal\n\u2022 After computing deviations and summing:\n\ud835\udc46\ud835\udc4a =\n6 3 \u22122\n3 5 \u22121\n\u22122 \u22121 4\n\nStep 5: Compute Between-Class Scatter \nMatrix (S\u1d66)\n\ud835\udc46\ud835\udc35 =  \ud835\udf071  \u2212  \ud835\udf072 (\ud835\udf071  \u2212  \ud835\udf072)\ud835\udc47\n\ud835\udf071 \u2212 \ud835\udf072 =\n4.5\n2.75\n\u22124.5\n\ud835\udc46\ud835\udc35 =\n20.25 12.38 \u221220.25\n12.38 7.56 \u221212.38\n\u221220.25 \u221212.38 20.25\nStep 6: Compute LDA Projection Vector (w)\n\u2022 LDA Optimization Objective\n\ud835\udc30 = \ud835\udc46\ud835\udc4a\n\u22121 \ud835\udf071 \u2212 \ud835\udf072\n\u2022 After computing the inverse:\n\ud835\udc30 =\n0.62\n0.38\n\u22120.69\n This vector defines the optimal direction that:\n\u2022 Maximizes class separation\n\u2022 Minimizes overlap\nStep 7: Project Images onto 1D LDA Axis\n\u2022 Each image is projected as:\n\ud835\udc66 = \ud835\udc30\ud835\udc47\ud835\udc31\n\u2022 Image from Class \u201c0\u201d\n\ud835\udc31 = 7 6 3\n\ud835\udc66 = 0.62 7 + 0.38 6 \u2212 0.69 3 = 4.45\n\u2022 Image from Class \u201c1\u201d\n\ud835\udc31 = 2 3 7\n\ud835\udc66 = 0.62 2 + 0.38 3 \u2212 0.69 7 = \u22122.41\nStep 8: Classification Rule\n\u2022 Choose a threshold \ud835\udc61(often the midpoint between projected means):\n\ud835\udc61 \u2248 1.0\n\u2022 Decision Rule\nIf \ud835\udc66 > \ud835\udc61 \u21d2 Class \u201c0\u201d\nIf \ud835\udc66 \u2264 \ud835\udc61 \u21d2 Class \u201c1\u201d\n\u201cLDA squeezes multi-feature images onto a single line where different classes \nare as far apart as possible.\u201d\n\nPython Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import \nLinearDiscriminantAnalysis\nfrom sklearn.datasets import load_iris\n# Load Iris data\nX, y = load_iris(return_X_y=True)\n# Select ONLY two classes (0 and 1) and two features\nX = X[y != 2][:, :2]   # sepal length & sepal width\ny = y[y != 2]\n# Fit LDA\nlda = LinearDiscriminantAnalysis(n_components=1)\nlda.fit(X, y)\n# Scatter plot of original data\nplt.scatter(X[y == 0, 0], X[y == 0, 1], label='Class 0')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], label='Class 1')\n# LDA direction (weight vector)\nw = lda.coef_[0]\nw = w / np.linalg.norm(w)  # normalize\n# Mean point\nmean = X.mean(axis=0)\n# Create LDA line\nx_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\ny_vals = mean[1] + (w[1] / w[0]) * (x_vals - mean[0])\nplt.plot(x_vals, y_vals, 'k--', label='LDA line')\n# Labels\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('LDA: Data Scatter with Discriminant Line')\nplt.legend()\nplt.show()\nAdvantages of LDA\n\u2022 Uses class label information\n\u2022 Good class separability\n\u2022 Reduces dimensionality\n\u2022 Works well for linearly separable data\nLimitations of LDA\n\u2022 Assumes:\n\u2022 Normal distribution\n\u2022 Equal covariance matrices\n\u2022 Not suitable for highly non-linear data\nKey Takeaways\n\u2022 LDA is a supervised dimensionality reduction technique\n\u2022 Maximizes class separability\n\u2022 Useful for both visualization and classification\n\u2022 Widely used in real-world ML applications"]